{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f5df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aula 10 - criar colunas no DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21cb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seção imports\n",
    "import findspark\n",
    "import os\n",
    "import pyspark.sql.functions as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bceb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d422206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este seção cria o APPNAME e configura o formato de saida da função show()\n",
    "spark = ( \n",
    "    SparkSession.builder.appName('ralves_sess_2')\n",
    "    .config (\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8601a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://INEX-5098.portoseguro.brasil:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ralves_sess_2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1da9de4d198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b211f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comando para parar uma seção SPARK\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb0ca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "#describe python version\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59709ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file shortcut \n",
    "trnfile2=r'C:/sources/LOGINS.parquet'\n",
    "\n",
    "#create dataframe\n",
    "df=spark.read.parquet(trnfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b0d0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cpf</th><th>email</th><th>senha</th><th>data_de_nascimento</th><th>estado</th><th>data_cadastro</th><th>ipv4</th><th>cor_favorita</th><th>profissao</th><th>telefone</th></tr>\n",
       "<tr><td>981.507.362-12</td><td>pedro-lucas53@gma...</td><td>+7^7E%xFBc</td><td>2006-12-18</td><td>RR</td><td>2023-02-26</td><td>99.107.250.210</td><td>Roxo</td><td>Jogador De Golfe</td><td>31 7785-4046</td></tr>\n",
       "<tr><td>493.705.168-75</td><td>rezendeisaac@hotm...</td><td>_O_2GRnGOe</td><td>1992-06-17</td><td>GO</td><td>2023-02-16</td><td>197.11.26.213</td><td>Ciano</td><td>Atleta De Arremes...</td><td>(031) 0803-6753</td></tr>\n",
       "<tr><td>398.471.625-73</td><td>felipepires@uol.c...</td><td>*Aw5EOAvy9</td><td>1921-11-11</td><td>MG</td><td>2023-01-02</td><td>181.90.63.58</td><td>Azul</td><td>Papiloscopista</td><td>11 9674-0553</td></tr>\n",
       "<tr><td>092.618.354-06</td><td>stellamoraes@bol....</td><td>mw0AWYAs#s</td><td>2021-06-01</td><td>AC</td><td>2023-01-08</td><td>26.121.127.94</td><td>Marrom</td><td>Aeromoça</td><td>+55 (071) 3033 9177</td></tr>\n",
       "<tr><td>509.427.136-99</td><td>wcarvalho@ig.com.br</td><td>pGD%!2Pq5X</td><td>1969-10-28</td><td>AP</td><td>2023-02-14</td><td>76.184.52.163</td><td>Laranja</td><td>Fonoaudiólogo</td><td>+55 (071) 6272 2468</td></tr>\n",
       "<tr><td>218.795.460-94</td><td>da-conceicaodavi-...</td><td>uhBbFxPA&amp;9</td><td>1986-05-19</td><td>MG</td><td>2023-03-07</td><td>192.93.0.24</td><td>Rosa</td><td>Taxista</td><td>+55 84 0652 9691</td></tr>\n",
       "<tr><td>715.836.940-48</td><td>efreitas@bol.com.br</td><td>s#q9VZt&amp;xl</td><td>2018-04-20</td><td>MG</td><td>2023-01-13</td><td>76.251.188.148</td><td>Branco</td><td>Produtor De Audio...</td><td>+55 (084) 1363 0052</td></tr>\n",
       "<tr><td>475.698.032-56</td><td>wnunes@bol.com.br</td><td>_8az1W%n7g</td><td>1996-05-12</td><td>SE</td><td>2023-02-04</td><td>139.196.176.154</td><td>Azul</td><td>Cadeirinha</td><td>(071) 1640-3388</td></tr>\n",
       "<tr><td>217.639.540-99</td><td>jribeiro@bol.com.br</td><td>MEf1X7fj_0</td><td>2021-10-05</td><td>PA</td><td>2023-03-02</td><td>71.22.224.5</td><td>Marrom</td><td>Geólogo</td><td>21 1432 4092</td></tr>\n",
       "<tr><td>261.938.750-77</td><td>murilo05@gmail.com</td><td>Te&amp;gO7GkKs</td><td>1917-01-05</td><td>MT</td><td>2023-02-21</td><td>136.54.123.165</td><td>Marrom</td><td>Técnico De Som</td><td>+55 (084) 5878-3346</td></tr>\n",
       "<tr><td>520.831.796-68</td><td>joaquim57@ig.com.br</td><td>&amp;2E1NY+ARc</td><td>1912-05-25</td><td>BA</td><td>2023-01-25</td><td>78.196.255.126</td><td>Rosa</td><td>Esteticista</td><td>41 7914-3753</td></tr>\n",
       "<tr><td>413.087.526-44</td><td>alexiada-rocha@ig...</td><td>@f@!Z!2c*2</td><td>1920-05-26</td><td>MS</td><td>2023-02-18</td><td>182.61.65.201</td><td>Laranja</td><td>Microfonista</td><td>(021) 3739-2944</td></tr>\n",
       "<tr><td>509.287.143-14</td><td>pmendes@gmail.com</td><td>M+^XDBfe(2</td><td>1938-03-11</td><td>RR</td><td>2023-01-19</td><td>61.234.208.17</td><td>Verde Claro</td><td>Fiscal</td><td>+55 (084) 5940-1932</td></tr>\n",
       "<tr><td>563.170.492-70</td><td>renan46@bol.com.br</td><td>Rp%2pVqfe$</td><td>1922-09-01</td><td>PA</td><td>2023-02-01</td><td>122.203.83.177</td><td>Verde Escuro</td><td>Contabilista</td><td>+55 41 6248 5773</td></tr>\n",
       "<tr><td>098.712.346-78</td><td>manuelada-rosa@ho...</td><td>#hEIEOztQ3</td><td>2013-01-10</td><td>SE</td><td>2023-01-07</td><td>175.18.73.211</td><td>Violeta</td><td>Seguidor De Compras</td><td>51 0278 0564</td></tr>\n",
       "<tr><td>345.709.261-34</td><td>usales@ig.com.br</td><td>b_8xaY$ozJ</td><td>1992-10-16</td><td>AM</td><td>2023-01-05</td><td>97.93.29.75</td><td>Rosa</td><td>Almirante</td><td>(084) 8993-4521</td></tr>\n",
       "<tr><td>318.254.906-51</td><td>pereiranicolas@ho...</td><td>YA9I85Wb+2</td><td>1945-11-18</td><td>MA</td><td>2023-01-24</td><td>182.195.33.137</td><td>Amarelo</td><td>Consultor De Moda</td><td>(084) 4591 3795</td></tr>\n",
       "<tr><td>610.287.453-62</td><td>cardosolivia@ig.c...</td><td>yXkDW7Ebh*</td><td>1915-11-15</td><td>GO</td><td>2023-01-22</td><td>195.194.63.184</td><td>Laranja</td><td>Barbeiro</td><td>+55 61 8967-9563</td></tr>\n",
       "<tr><td>615.790.842-49</td><td>brenda39@ig.com.br</td><td>XS$84Hpsnw</td><td>1964-02-01</td><td>RJ</td><td>2023-02-16</td><td>21.252.226.29</td><td>Branco</td><td>Promotor De Vendas</td><td>+55 (021) 4751 2004</td></tr>\n",
       "<tr><td>624.095.138-24</td><td>luizapeixoto@gmai...</td><td>TbKy82Kda$</td><td>1959-02-01</td><td>DF</td><td>2023-02-25</td><td>105.107.23.13</td><td>Ciano</td><td>Implantodontista</td><td>84 1293 1906</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+\n",
       "|           cpf|               email|     senha|data_de_nascimento|estado|data_cadastro|           ipv4|cor_favorita|           profissao|           telefone|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+\n",
       "|981.507.362-12|pedro-lucas53@gma...|+7^7E%xFBc|        2006-12-18|    RR|   2023-02-26| 99.107.250.210|        Roxo|    Jogador De Golfe|       31 7785-4046|\n",
       "|493.705.168-75|rezendeisaac@hotm...|_O_2GRnGOe|        1992-06-17|    GO|   2023-02-16|  197.11.26.213|       Ciano|Atleta De Arremes...|    (031) 0803-6753|\n",
       "|398.471.625-73|felipepires@uol.c...|*Aw5EOAvy9|        1921-11-11|    MG|   2023-01-02|   181.90.63.58|        Azul|      Papiloscopista|       11 9674-0553|\n",
       "|092.618.354-06|stellamoraes@bol....|mw0AWYAs#s|        2021-06-01|    AC|   2023-01-08|  26.121.127.94|      Marrom|            Aeromoça|+55 (071) 3033 9177|\n",
       "|509.427.136-99| wcarvalho@ig.com.br|pGD%!2Pq5X|        1969-10-28|    AP|   2023-02-14|  76.184.52.163|     Laranja|       Fonoaudiólogo|+55 (071) 6272 2468|\n",
       "|218.795.460-94|da-conceicaodavi-...|uhBbFxPA&9|        1986-05-19|    MG|   2023-03-07|    192.93.0.24|        Rosa|             Taxista|   +55 84 0652 9691|\n",
       "|715.836.940-48| efreitas@bol.com.br|s#q9VZt&xl|        2018-04-20|    MG|   2023-01-13| 76.251.188.148|      Branco|Produtor De Audio...|+55 (084) 1363 0052|\n",
       "|475.698.032-56|   wnunes@bol.com.br|_8az1W%n7g|        1996-05-12|    SE|   2023-02-04|139.196.176.154|        Azul|          Cadeirinha|    (071) 1640-3388|\n",
       "|217.639.540-99| jribeiro@bol.com.br|MEf1X7fj_0|        2021-10-05|    PA|   2023-03-02|    71.22.224.5|      Marrom|             Geólogo|       21 1432 4092|\n",
       "|261.938.750-77|  murilo05@gmail.com|Te&gO7GkKs|        1917-01-05|    MT|   2023-02-21| 136.54.123.165|      Marrom|      Técnico De Som|+55 (084) 5878-3346|\n",
       "|520.831.796-68| joaquim57@ig.com.br|&2E1NY+ARc|        1912-05-25|    BA|   2023-01-25| 78.196.255.126|        Rosa|         Esteticista|       41 7914-3753|\n",
       "|413.087.526-44|alexiada-rocha@ig...|@f@!Z!2c*2|        1920-05-26|    MS|   2023-02-18|  182.61.65.201|     Laranja|        Microfonista|    (021) 3739-2944|\n",
       "|509.287.143-14|   pmendes@gmail.com|M+^XDBfe(2|        1938-03-11|    RR|   2023-01-19|  61.234.208.17| Verde Claro|              Fiscal|+55 (084) 5940-1932|\n",
       "|563.170.492-70|  renan46@bol.com.br|Rp%2pVqfe$|        1922-09-01|    PA|   2023-02-01| 122.203.83.177|Verde Escuro|        Contabilista|   +55 41 6248 5773|\n",
       "|098.712.346-78|manuelada-rosa@ho...|#hEIEOztQ3|        2013-01-10|    SE|   2023-01-07|  175.18.73.211|     Violeta| Seguidor De Compras|       51 0278 0564|\n",
       "|345.709.261-34|    usales@ig.com.br|b_8xaY$ozJ|        1992-10-16|    AM|   2023-01-05|    97.93.29.75|        Rosa|           Almirante|    (084) 8993-4521|\n",
       "|318.254.906-51|pereiranicolas@ho...|YA9I85Wb+2|        1945-11-18|    MA|   2023-01-24| 182.195.33.137|     Amarelo|   Consultor De Moda|    (084) 4591 3795|\n",
       "|610.287.453-62|cardosolivia@ig.c...|yXkDW7Ebh*|        1915-11-15|    GO|   2023-01-22| 195.194.63.184|     Laranja|            Barbeiro|   +55 61 8967-9563|\n",
       "|615.790.842-49|  brenda39@ig.com.br|XS$84Hpsnw|        1964-02-01|    RJ|   2023-02-16|  21.252.226.29|      Branco|  Promotor De Vendas|+55 (021) 4751 2004|\n",
       "|624.095.138-24|luizapeixoto@gmai...|TbKy82Kda$|        1959-02-01|    DF|   2023-02-25|  105.107.23.13|       Ciano|    Implantodontista|       84 1293 1906|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f7507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        builtins.str\n",
      "        PandasUDFType\n",
      "    \n",
      "    class PandasUDFType(builtins.object)\n",
      "     |  Pandas UDF Types. See :meth:`pyspark.sql.functions.pandas_udf`.\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  GROUPED_AGG = 202\n",
      "     |  \n",
      "     |  GROUPED_MAP = 201\n",
      "     |  \n",
      "     |  SCALAR = 200\n",
      "    \n",
      "    basestring = class str(object)\n",
      "     |  str(object='') -> str\n",
      "     |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      "     |  \n",
      "     |  Create a new string object from the given object. If encoding or\n",
      "     |  errors is specified, then the object must expose a data buffer\n",
      "     |  that will be decoded using the given encoding and error handler.\n",
      "     |  Otherwise, returns the result of object.__str__() (if defined)\n",
      "     |  or repr(object).\n",
      "     |  encoding defaults to sys.getdefaultencoding().\n",
      "     |  errors defaults to 'strict'.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __format__(self, format_spec, /)\n",
      "     |      Return a formatted version of the string as described by format_spec.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __getnewargs__(...)\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mod__(self, value, /)\n",
      "     |      Return self%value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmod__(self, value, /)\n",
      "     |      Return value%self.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  __sizeof__(self, /)\n",
      "     |      Return the size of the string in memory, in bytes.\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  capitalize(self, /)\n",
      "     |      Return a capitalized version of the string.\n",
      "     |      \n",
      "     |      More specifically, make the first character have upper case and the rest lower\n",
      "     |      case.\n",
      "     |  \n",
      "     |  casefold(self, /)\n",
      "     |      Return a version of the string suitable for caseless comparisons.\n",
      "     |  \n",
      "     |  center(self, width, fillchar=' ', /)\n",
      "     |      Return a centered string of length width.\n",
      "     |      \n",
      "     |      Padding is done using the specified fill character (default is a space).\n",
      "     |  \n",
      "     |  count(...)\n",
      "     |      S.count(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the number of non-overlapping occurrences of substring sub in\n",
      "     |      string S[start:end].  Optional arguments start and end are\n",
      "     |      interpreted as in slice notation.\n",
      "     |  \n",
      "     |  encode(self, /, encoding='utf-8', errors='strict')\n",
      "     |      Encode the string using the codec registered for encoding.\n",
      "     |      \n",
      "     |      encoding\n",
      "     |        The encoding in which to encode the string.\n",
      "     |      errors\n",
      "     |        The error handling scheme to use for encoding errors.\n",
      "     |        The default is 'strict' meaning that encoding errors raise a\n",
      "     |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
      "     |        'xmlcharrefreplace' as well as any other name registered with\n",
      "     |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
      "     |  \n",
      "     |  endswith(...)\n",
      "     |      S.endswith(suffix[, start[, end]]) -> bool\n",
      "     |      \n",
      "     |      Return True if S ends with the specified suffix, False otherwise.\n",
      "     |      With optional start, test S beginning at that position.\n",
      "     |      With optional end, stop comparing S at that position.\n",
      "     |      suffix can also be a tuple of strings to try.\n",
      "     |  \n",
      "     |  expandtabs(self, /, tabsize=8)\n",
      "     |      Return a copy where all tab characters are expanded using spaces.\n",
      "     |      \n",
      "     |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      S.find(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the lowest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Return -1 on failure.\n",
      "     |  \n",
      "     |  format(...)\n",
      "     |      S.format(*args, **kwargs) -> str\n",
      "     |      \n",
      "     |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      "     |      The substitutions are identified by braces ('{' and '}').\n",
      "     |  \n",
      "     |  format_map(...)\n",
      "     |      S.format_map(mapping) -> str\n",
      "     |      \n",
      "     |      Return a formatted version of S, using substitutions from mapping.\n",
      "     |      The substitutions are identified by braces ('{' and '}').\n",
      "     |  \n",
      "     |  index(...)\n",
      "     |      S.index(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the lowest index in S where substring sub is found, \n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Raises ValueError when the substring is not found.\n",
      "     |  \n",
      "     |  isalnum(self, /)\n",
      "     |      Return True if the string is an alpha-numeric string, False otherwise.\n",
      "     |      \n",
      "     |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
      "     |      there is at least one character in the string.\n",
      "     |  \n",
      "     |  isalpha(self, /)\n",
      "     |      Return True if the string is an alphabetic string, False otherwise.\n",
      "     |      \n",
      "     |      A string is alphabetic if all characters in the string are alphabetic and there\n",
      "     |      is at least one character in the string.\n",
      "     |  \n",
      "     |  isascii(self, /)\n",
      "     |      Return True if all characters in the string are ASCII, False otherwise.\n",
      "     |      \n",
      "     |      ASCII characters have code points in the range U+0000-U+007F.\n",
      "     |      Empty string is ASCII too.\n",
      "     |  \n",
      "     |  isdecimal(self, /)\n",
      "     |      Return True if the string is a decimal string, False otherwise.\n",
      "     |      \n",
      "     |      A string is a decimal string if all characters in the string are decimal and\n",
      "     |      there is at least one character in the string.\n",
      "     |  \n",
      "     |  isdigit(self, /)\n",
      "     |      Return True if the string is a digit string, False otherwise.\n",
      "     |      \n",
      "     |      A string is a digit string if all characters in the string are digits and there\n",
      "     |      is at least one character in the string.\n",
      "     |  \n",
      "     |  isidentifier(self, /)\n",
      "     |      Return True if the string is a valid Python identifier, False otherwise.\n",
      "     |      \n",
      "     |      Use keyword.iskeyword() to test for reserved identifiers such as \"def\" and\n",
      "     |      \"class\".\n",
      "     |  \n",
      "     |  islower(self, /)\n",
      "     |      Return True if the string is a lowercase string, False otherwise.\n",
      "     |      \n",
      "     |      A string is lowercase if all cased characters in the string are lowercase and\n",
      "     |      there is at least one cased character in the string.\n",
      "     |  \n",
      "     |  isnumeric(self, /)\n",
      "     |      Return True if the string is a numeric string, False otherwise.\n",
      "     |      \n",
      "     |      A string is numeric if all characters in the string are numeric and there is at\n",
      "     |      least one character in the string.\n",
      "     |  \n",
      "     |  isprintable(self, /)\n",
      "     |      Return True if the string is printable, False otherwise.\n",
      "     |      \n",
      "     |      A string is printable if all of its characters are considered printable in\n",
      "     |      repr() or if it is empty.\n",
      "     |  \n",
      "     |  isspace(self, /)\n",
      "     |      Return True if the string is a whitespace string, False otherwise.\n",
      "     |      \n",
      "     |      A string is whitespace if all characters in the string are whitespace and there\n",
      "     |      is at least one character in the string.\n",
      "     |  \n",
      "     |  istitle(self, /)\n",
      "     |      Return True if the string is a title-cased string, False otherwise.\n",
      "     |      \n",
      "     |      In a title-cased string, upper- and title-case characters may only\n",
      "     |      follow uncased characters and lowercase characters only cased ones.\n",
      "     |  \n",
      "     |  isupper(self, /)\n",
      "     |      Return True if the string is an uppercase string, False otherwise.\n",
      "     |      \n",
      "     |      A string is uppercase if all cased characters in the string are uppercase and\n",
      "     |      there is at least one cased character in the string.\n",
      "     |  \n",
      "     |  join(self, iterable, /)\n",
      "     |      Concatenate any number of strings.\n",
      "     |      \n",
      "     |      The string whose method is called is inserted in between each given string.\n",
      "     |      The result is returned as a new string.\n",
      "     |      \n",
      "     |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
      "     |  \n",
      "     |  ljust(self, width, fillchar=' ', /)\n",
      "     |      Return a left-justified string of length width.\n",
      "     |      \n",
      "     |      Padding is done using the specified fill character (default is a space).\n",
      "     |  \n",
      "     |  lower(self, /)\n",
      "     |      Return a copy of the string converted to lowercase.\n",
      "     |  \n",
      "     |  lstrip(self, chars=None, /)\n",
      "     |      Return a copy of the string with leading whitespace removed.\n",
      "     |      \n",
      "     |      If chars is given and not None, remove characters in chars instead.\n",
      "     |  \n",
      "     |  partition(self, sep, /)\n",
      "     |      Partition the string into three parts using the given separator.\n",
      "     |      \n",
      "     |      This will search for the separator in the string.  If the separator is found,\n",
      "     |      returns a 3-tuple containing the part before the separator, the separator\n",
      "     |      itself, and the part after it.\n",
      "     |      \n",
      "     |      If the separator is not found, returns a 3-tuple containing the original string\n",
      "     |      and two empty strings.\n",
      "     |  \n",
      "     |  replace(self, old, new, count=-1, /)\n",
      "     |      Return a copy with all occurrences of substring old replaced by new.\n",
      "     |      \n",
      "     |        count\n",
      "     |          Maximum number of occurrences to replace.\n",
      "     |          -1 (the default value) means replace all occurrences.\n",
      "     |      \n",
      "     |      If the optional argument count is given, only the first count occurrences are\n",
      "     |      replaced.\n",
      "     |  \n",
      "     |  rfind(...)\n",
      "     |      S.rfind(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the highest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Return -1 on failure.\n",
      "     |  \n",
      "     |  rindex(...)\n",
      "     |      S.rindex(sub[, start[, end]]) -> int\n",
      "     |      \n",
      "     |      Return the highest index in S where substring sub is found,\n",
      "     |      such that sub is contained within S[start:end].  Optional\n",
      "     |      arguments start and end are interpreted as in slice notation.\n",
      "     |      \n",
      "     |      Raises ValueError when the substring is not found.\n",
      "     |  \n",
      "     |  rjust(self, width, fillchar=' ', /)\n",
      "     |      Return a right-justified string of length width.\n",
      "     |      \n",
      "     |      Padding is done using the specified fill character (default is a space).\n",
      "     |  \n",
      "     |  rpartition(self, sep, /)\n",
      "     |      Partition the string into three parts using the given separator.\n",
      "     |      \n",
      "     |      This will search for the separator in the string, starting at the end. If\n",
      "     |      the separator is found, returns a 3-tuple containing the part before the\n",
      "     |      separator, the separator itself, and the part after it.\n",
      "     |      \n",
      "     |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
      "     |      and the original string.\n",
      "     |  \n",
      "     |  rsplit(self, /, sep=None, maxsplit=-1)\n",
      "     |      Return a list of the words in the string, using sep as the delimiter string.\n",
      "     |      \n",
      "     |        sep\n",
      "     |          The delimiter according which to split the string.\n",
      "     |          None (the default value) means split according to any whitespace,\n",
      "     |          and discard empty strings from the result.\n",
      "     |        maxsplit\n",
      "     |          Maximum number of splits to do.\n",
      "     |          -1 (the default value) means no limit.\n",
      "     |      \n",
      "     |      Splits are done starting at the end of the string and working to the front.\n",
      "     |  \n",
      "     |  rstrip(self, chars=None, /)\n",
      "     |      Return a copy of the string with trailing whitespace removed.\n",
      "     |      \n",
      "     |      If chars is given and not None, remove characters in chars instead.\n",
      "     |  \n",
      "     |  split(self, /, sep=None, maxsplit=-1)\n",
      "     |      Return a list of the words in the string, using sep as the delimiter string.\n",
      "     |      \n",
      "     |      sep\n",
      "     |        The delimiter according which to split the string.\n",
      "     |        None (the default value) means split according to any whitespace,\n",
      "     |        and discard empty strings from the result.\n",
      "     |      maxsplit\n",
      "     |        Maximum number of splits to do.\n",
      "     |        -1 (the default value) means no limit.\n",
      "     |  \n",
      "     |  splitlines(self, /, keepends=False)\n",
      "     |      Return a list of the lines in the string, breaking at line boundaries.\n",
      "     |      \n",
      "     |      Line breaks are not included in the resulting list unless keepends is given and\n",
      "     |      true.\n",
      "     |  \n",
      "     |  startswith(...)\n",
      "     |      S.startswith(prefix[, start[, end]]) -> bool\n",
      "     |      \n",
      "     |      Return True if S starts with the specified prefix, False otherwise.\n",
      "     |      With optional start, test S beginning at that position.\n",
      "     |      With optional end, stop comparing S at that position.\n",
      "     |      prefix can also be a tuple of strings to try.\n",
      "     |  \n",
      "     |  strip(self, chars=None, /)\n",
      "     |      Return a copy of the string with leading and trailing whitespace remove.\n",
      "     |      \n",
      "     |      If chars is given and not None, remove characters in chars instead.\n",
      "     |  \n",
      "     |  swapcase(self, /)\n",
      "     |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
      "     |  \n",
      "     |  title(self, /)\n",
      "     |      Return a version of the string where each word is titlecased.\n",
      "     |      \n",
      "     |      More specifically, words start with uppercased characters and all remaining\n",
      "     |      cased characters have lower case.\n",
      "     |  \n",
      "     |  translate(self, table, /)\n",
      "     |      Replace each character in the string using the given translation table.\n",
      "     |      \n",
      "     |        table\n",
      "     |          Translation table, which must be a mapping of Unicode ordinals to\n",
      "     |          Unicode ordinals, strings, or None.\n",
      "     |      \n",
      "     |      The table must implement lookup/indexing via __getitem__, for instance a\n",
      "     |      dictionary or list.  If this operation raises LookupError, the character is\n",
      "     |      left untouched.  Characters mapped to None are deleted.\n",
      "     |  \n",
      "     |  upper(self, /)\n",
      "     |      Return a copy of the string converted to uppercase.\n",
      "     |  \n",
      "     |  zfill(self, width, /)\n",
      "     |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
      "     |      \n",
      "     |      The string is never truncated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  maketrans(x, y=None, z=None, /)\n",
      "     |      Return a translation table usable for str.translate().\n",
      "     |      \n",
      "     |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      "     |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      "     |      Character keys will be then converted to ordinals.\n",
      "     |      If there are two arguments, they must be strings of equal length, and\n",
      "     |      in the resulting dictionary, each character in x will be mapped to the\n",
      "     |      character at the same position in y. If there is a third argument, it\n",
      "     |      must be a string, whose characters will be mapped to None in the result.\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col)\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    acos(col)\n",
      "        :return: inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    add_months(start, months)\n",
      "        Returns the date that is `months` months after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    approxCountDistinct(col, rsd=None)\n",
      "        .. note:: Deprecated in 2.1, use :func:`approx_count_distinct` instead.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    approx_count_distinct(col, rsd=None)\n",
      "        Aggregate function: returns a new :class:`Column` for approximate distinct count of\n",
      "        column `col`.\n",
      "        \n",
      "        :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\n",
      "            efficient to use :func:`countDistinct`\n",
      "        \n",
      "        >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\n",
      "        [Row(distinct_ages=2)]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    array(*cols)\n",
      "        Creates a new array column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that have\n",
      "            the same data type.\n",
      "        \n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    array_contains(col, value)\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        :param col: name of column containing array\n",
      "        :param value: value to check for in array\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    array_distinct(col)\n",
      "        Collection function: removes duplicate values from the array.\n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
      "        >>> df.select(array_distinct(df.data)).collect()\n",
      "        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_except(col1, col2)\n",
      "        Collection function: returns an array of the elements in col1 but not in col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        :param col1: name of column containing array\n",
      "        :param col2: name of column containing array\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_except(df.c1, df.c2)).collect()\n",
      "        [Row(array_except(c1, c2)=['b'])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_intersect(col1, col2)\n",
      "        Collection function: returns an array of the elements in the intersection of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        :param col1: name of column containing array\n",
      "        :param col2: name of column containing array\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n",
      "        [Row(array_intersect(c1, c2)=['a', 'c'])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_join(col, delimiter, null_replacement=None)\n",
      "        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n",
      "        `null_replacement` if set, otherwise they are ignored.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
      "        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a')]\n",
      "        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_max(col)\n",
      "        Collection function: returns the maximum value of the array.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_max(df.data).alias('max')).collect()\n",
      "        [Row(max=3), Row(max=10)]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_min(col)\n",
      "        Collection function: returns the minimum value of the array.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_min(df.data).alias('min')).collect()\n",
      "        [Row(min=1), Row(min=-1)]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_position(col, value)\n",
      "        Collection function: Locates the position of the first occurrence of the given value\n",
      "        in the given array. Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if the given\n",
      "            value could not be found in the array.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_position(df.data, \"a\")).collect()\n",
      "        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_remove(col, element)\n",
      "        Collection function: Remove all elements that equal to element from the given array.\n",
      "        \n",
      "        :param col: name of column containing array\n",
      "        :param element: element to be removed from the array\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
      "        >>> df.select(array_remove(df.data, 1)).collect()\n",
      "        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_repeat(col, count)\n",
      "        Collection function: creates an array containing a column repeated count times.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab',)], ['data'])\n",
      "        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n",
      "        [Row(r=['ab', 'ab', 'ab'])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_sort(col)\n",
      "        Collection function: sorts the input array in ascending order. The elements of the input array\n",
      "        must be orderable. Null elements will be placed at the end of the returned array.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    array_union(col1, col2)\n",
      "        Collection function: returns an array of the elements in the union of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        :param col1: name of column containing array\n",
      "        :param col2: name of column containing array\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_union(df.c1, df.c2)).collect()\n",
      "        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    arrays_overlap(a1, a2)\n",
      "        Collection function: returns true if the arrays contain any common non-null element; if not,\n",
      "        returns null if both the arrays are non-empty and any of them contains a null element; returns\n",
      "        false otherwise.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
      "        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n",
      "        [Row(overlap=True), Row(overlap=False)]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    arrays_zip(*cols)\n",
      "        Collection function: Returns a merged array of structs in which the N-th struct contains all\n",
      "        N-th values of input arrays.\n",
      "        \n",
      "        :param cols: columns of arrays to be merged.\n",
      "        \n",
      "        >>> from pyspark.sql.functions import arrays_zip\n",
      "        >>> df = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n",
      "        >>> df.select(arrays_zip(df.vals1, df.vals2).alias('zipped')).collect()\n",
      "        [Row(zipped=[Row(vals1=1, vals2=2), Row(vals1=2, vals2=3), Row(vals1=3, vals2=4)])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    asc(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    asc_nulls_first(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name, and null values return before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    asc_nulls_last(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    ascii(col)\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asin(col)\n",
      "        :return: inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan(col)\n",
      "        :return: inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan2(col1, col2)\n",
      "        :param col1: coordinate on y-axis\n",
      "        :param col2: coordinate on x-axis\n",
      "        :return: the `theta` component of the point\n",
      "           (`r`, `theta`)\n",
      "           in polar coordinates that corresponds to the point\n",
      "           (`x`, `y`) in Cartesian coordinates,\n",
      "           as if computed by `java.lang.Math.atan2()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    avg(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    base64(col)\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bin(col)\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        >>> df.select(bin(df.age).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bitwiseNOT(col)\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    broadcast(df)\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    bround(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cbrt(col)\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    ceil(col)\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    coalesce(*cols)\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    col(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    collect_list(col)\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. note:: The function is non-deterministic because the order of collected results depends\n",
      "            on order of rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    collect_set(col)\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. note:: The function is non-deterministic because the order of collected results depends\n",
      "            on order of rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_set('age')).collect()\n",
      "        [Row(collect_set(age)=[5, 2])]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    column(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    concat(*cols)\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        The function works with strings, binary and compatible array columns.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd123')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "        >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\n",
      "        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    concat_ws(sep, *cols)\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    conv(col, fromBase, toBase)\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    corr(col1, col2)\n",
      "        Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    cos(col)\n",
      "        :param col: angle in radians\n",
      "        :return: cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    cosh(col)\n",
      "        :param col: hyperbolic angle\n",
      "        :return: hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    count(col)\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    countDistinct(col, *cols)\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    covar_pop(col1, col2)\n",
      "        Returns a new :class:`Column` for the population covariance of ``col1`` and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    covar_samp(col1, col2)\n",
      "        Returns a new :class:`Column` for the sample covariance of ``col1`` and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    crc32(col)\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    create_map(*cols)\n",
      "        Creates a new map column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cume_dist()\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    current_date()\n",
      "        Returns the current date as a :class:`DateType` column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    current_timestamp()\n",
      "        Returns the current timestamp as a :class:`TimestampType` column.\n",
      "    \n",
      "    date_add(start, days)\n",
      "        Returns the date that is `days` days after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_format(date, format)\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of the Java class `java.text.SimpleDateFormat` can be used.\n",
      "        \n",
      "        .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n",
      "            specialized implementation.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_sub(start, days)\n",
      "        Returns the date that is `days` days before `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_trunc(format, timestamp)\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n",
      "            'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    datediff(end, start)\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofmonth(col)\n",
      "        Extract the day of the month of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofweek(col)\n",
      "        Extract the day of the week of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    dayofyear(col)\n",
      "        Extract the day of the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    decode(col, charset)\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    degrees(col)\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        :param col: angle in radians\n",
      "        :return: angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    dense_rank()\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    desc(col)\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    desc_nulls_first(col)\n",
      "        Returns a sort expression based on the descending order of the given column name, and null values appear before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    desc_nulls_last(col)\n",
      "        Returns a sort expression based on the descending order of the given column name, and null values appear after non-null values\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    element_at(col, extraction)\n",
      "        Collection function: Returns element of array at given index in extraction if col is array.\n",
      "        Returns value for the given key in extraction if col is map.\n",
      "        \n",
      "        :param col: name of column containing array or map\n",
      "        :param extraction: index to check for in array or key to check for in map\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(element_at(df.data, 1)).collect()\n",
      "        [Row(element_at(data, 1)='a'), Row(element_at(data, 1)=None)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], ['data'])\n",
      "        >>> df.select(element_at(df.data, lit(\"a\"))).collect()\n",
      "        [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    encode(col, charset)\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    exp(col)\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode_outer(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|null| null|\n",
      "        |  3|      null|null| null|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|[x -> 1.0]| foo|\n",
      "        |  1|[x -> 1.0]| bar|\n",
      "        |  2|        []|null|\n",
      "        |  3|      null|null|\n",
      "        +---+----------+----+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    expm1(col)\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expr(str)\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        >>> df.select(expr(\"length(name)\")).collect()\n",
      "        [Row(length(name)=5), Row(length(name)=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    factorial(col)\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    first(col, ignorenulls=False)\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. note:: The function is non-deterministic because its results depends on order of rows which\n",
      "            may be non-deterministic after a shuffle.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    flatten(col)\n",
      "        Collection function: creates a single array from an array of arrays.\n",
      "        If a structure of nested arrays is deeper than two levels,\n",
      "        only one level of nesting is removed.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
      "        >>> df.select(flatten(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, 4, 5, 6]), Row(r=None)]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    floor(col)\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    format_number(col, d)\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    format_string(format, *cols)\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        :param format: string that can contain embedded format tags and used as result column's value\n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions to\n",
      "            be used in formatting\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_json(col, schema, options={})\n",
      "        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n",
      "        as keys type, :class:`StructType` or :class:`ArrayType` with\n",
      "        the specified schema. Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param schema: a StructType or ArrayType of StructType to use when parsing the json column.\n",
      "        :param options: options to control parsing. accepts the same options as the json datasource\n",
      "        \n",
      "        .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also\n",
      "                  supported for ``schema``.\n",
      "        \n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
      "        [Row(json={'a': 1})]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> data = [(1, '''[1, 2, 3]''')]\n",
      "        >>> schema = ArrayType(IntegerType())\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[1, 2, 3])]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_utc_timestamp(timestamp, tz)\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n",
      "        renders that timestamp as a timestamp in the given time zone.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n",
      "        the given timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        :param timestamp: the column that contains timestamps\n",
      "        :param tz: a string that has the ID of timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc\n",
      "        \n",
      "        .. versionchanged:: 2.4\n",
      "           `tz` can take a :class:`Column` containing timezone ID strings.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    get_json_object(col, path)\n",
      "        Extracts json object from a json string based on json path specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param path: path to the json object to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    greatest(*cols)\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    grouping(col)\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    grouping_id(*cols)\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. note:: The list of columns should match with grouping columns exactly, or empty (means all\n",
      "            the grouping columns).\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+-------------+--------+\n",
      "        | name|grouping_id()|sum(age)|\n",
      "        +-----+-------------+--------+\n",
      "        | null|            1|       7|\n",
      "        |Alice|            0|       2|\n",
      "        |  Bob|            0|       5|\n",
      "        +-----+-------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hash(*cols)\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n",
      "        [Row(hash=-757602832)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hex(col)\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hour(col)\n",
      "        Extract the hours of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hypot(col1, col2)\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    initcap(col)\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    input_file_name()\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    instr(str, substr)\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    isnan(col)\n",
      "        An expression that returns true iff the column is NaN.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    isnull(col)\n",
      "        An expression that returns true iff the column is null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    json_tuple(col, *fields)\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param fields: list of fields to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    kurtosis(col)\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    lag(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    last(col, ignorenulls=False)\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. note:: The function is non-deterministic because its results depends on order of rows\n",
      "            which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    last_day(date)\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lead(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    least(*cols)\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    length(col)\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    levenshtein(left, right)\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lit(col)\n",
      "        Creates a :class:`Column` of literal value.\n",
      "        \n",
      "        >>> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n",
      "        [Row(height=5, spark_user=True)]\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    locate(substr, str, pos=1)\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        :param substr: a string\n",
      "        :param str: a Column of :class:`pyspark.sql.types.StringType`\n",
      "        :param pos: start position (zero based)\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log(arg1, arg2=None)\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "        ['0.30102', '0.69897']\n",
      "        \n",
      "        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "        ['0.69314', '1.60943']\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log10(col)\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log1p(col)\n",
      "        Computes the natural logarithm of the given value plus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log2(col)\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n",
      "        [Row(log2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lower(col)\n",
      "        Converts a string column to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lpad(col, len, pad)\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ltrim(col)\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    map_concat(*cols)\n",
      "        Returns the union of all the given maps.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions\n",
      "        \n",
      "        >>> from pyspark.sql.functions import map_concat\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c', 1, 'd') as map2\")\n",
      "        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n",
      "        +--------------------------------+\n",
      "        |map3                            |\n",
      "        +--------------------------------+\n",
      "        |[1 -> a, 2 -> b, 3 -> c, 1 -> d]|\n",
      "        +--------------------------------+\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    map_from_arrays(col1, col2)\n",
      "        Creates a new map from two arrays.\n",
      "        \n",
      "        :param col1: name of column containing a set of keys. All elements should not be null\n",
      "        :param col2: name of column containing a set of values\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
      "        >>> df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |[2 -> a, 5 -> b]|\n",
      "        +----------------+\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    map_from_entries(col)\n",
      "        Collection function: Returns a map created from the given array of entries.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> from pyspark.sql.functions import map_from_entries\n",
      "        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
      "        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |[1 -> a, 2 -> b]|\n",
      "        +----------------+\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    map_keys(col)\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    map_values(col)\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    max(col)\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    md5(col)\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    mean(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    min(col)\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    minute(col)\n",
      "        Extract the minutes of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    monotonically_increasing_id()\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        .. note:: The function is non-deterministic because its result depends on partition IDs.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    month(col)\n",
      "         Extract the month of a given date as integer.\n",
      "        \n",
      "         >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "         >>> df.select(month('dt').alias('month')).collect()\n",
      "         [Row(month=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    months_between(date1, date2, roundOff=True)\n",
      "        Returns number of months between dates date1 and date2.\n",
      "        If date1 is later than date2, then the result is positive.\n",
      "        If date1 and date2 are on the same day of month, or both are the last day of month,\n",
      "        returns an integer (time of day will be ignored).\n",
      "        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.94959677)]\n",
      "        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "        [Row(months=3.9495967741935485)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    nanvl(col1, col2)\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    next_day(date, dayOfWeek)\n",
      "        Returns the first date which is later than the value of the date column.\n",
      "        \n",
      "        Day of the week parameter is case insensitive, and accepts:\n",
      "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ntile(n)\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        :param n: an integer\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    pandas_udf(f=None, returnType=None, functionType=None)\n",
      "        Creates a vectorized user defined function (UDF).\n",
      "        \n",
      "        :param f: user-defined function. A python function if used as a standalone function\n",
      "        :param returnType: the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n",
      "                             Default: SCALAR.\n",
      "        \n",
      "        .. note:: Experimental\n",
      "        \n",
      "        The function type of the UDF can be one of the following:\n",
      "        \n",
      "        1. SCALAR\n",
      "        \n",
      "           A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\n",
      "           The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\n",
      "        \n",
      "           :class:`MapType`, :class:`StructType` are currently not supported as output types.\n",
      "        \n",
      "           Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\n",
      "           :meth:`pyspark.sql.DataFrame.select`.\n",
      "        \n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> from pyspark.sql.types import IntegerType, StringType\n",
      "           >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\n",
      "           >>> @pandas_udf(StringType())  # doctest: +SKIP\n",
      "           ... def to_upper(s):\n",
      "           ...     return s.str.upper()\n",
      "           ...\n",
      "           >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n",
      "           ... def add_one(x):\n",
      "           ...     return x + 1\n",
      "           ...\n",
      "           >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\n",
      "           ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\n",
      "           >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\n",
      "           ...     .show()  # doctest: +SKIP\n",
      "           +----------+--------------+------------+\n",
      "           |slen(name)|to_upper(name)|add_one(age)|\n",
      "           +----------+--------------+------------+\n",
      "           |         8|      JOHN DOE|          22|\n",
      "           +----------+--------------+------------+\n",
      "        \n",
      "           .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input\n",
      "               column, but is the length of an internal batch used for each call to the function.\n",
      "               Therefore, this can be used, for example, to ensure the length of each returned\n",
      "               `pandas.Series`, and can not be used as the column length.\n",
      "        \n",
      "        2. GROUPED_MAP\n",
      "        \n",
      "           A grouped map UDF defines transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`\n",
      "           The returnType should be a :class:`StructType` describing the schema of the returned\n",
      "           `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\n",
      "           the field names in the defined returnType schema if specified as strings, or match the\n",
      "           field data types by position if not strings, e.g. integer indices.\n",
      "           The length of the returned `pandas.DataFrame` can be arbitrary.\n",
      "        \n",
      "           Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.\n",
      "        \n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> df = spark.createDataFrame(\n",
      "           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "           ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "           >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      "           ... def normalize(pdf):\n",
      "           ...     v = pdf.v\n",
      "           ...     return pdf.assign(v=(v - v.mean()) / v.std())\n",
      "           >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\n",
      "           +---+-------------------+\n",
      "           | id|                  v|\n",
      "           +---+-------------------+\n",
      "           |  1|-0.7071067811865475|\n",
      "           |  1| 0.7071067811865475|\n",
      "           |  2|-0.8320502943378437|\n",
      "           |  2|-0.2773500981126146|\n",
      "           |  2| 1.1094003924504583|\n",
      "           +---+-------------------+\n",
      "        \n",
      "           Alternatively, the user can define a function that takes two arguments.\n",
      "           In this case, the grouping key(s) will be passed as the first argument and the data will\n",
      "           be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\n",
      "           data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\n",
      "           as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\n",
      "           This is useful when the user does not want to hardcode grouping key(s) in the function.\n",
      "        \n",
      "           >>> import pandas as pd  # doctest: +SKIP\n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> df = spark.createDataFrame(\n",
      "           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "           ...     (\"id\", \"v\"))  # doctest: +SKIP\n",
      "           >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      "           ... def mean_udf(key, pdf):\n",
      "           ...     # key is a tuple of one numpy.int64, which is the value\n",
      "           ...     # of 'id' for the current group\n",
      "           ...     return pd.DataFrame([key + (pdf.v.mean(),)])\n",
      "           >>> df.groupby('id').apply(mean_udf).show()  # doctest: +SKIP\n",
      "           +---+---+\n",
      "           | id|  v|\n",
      "           +---+---+\n",
      "           |  1|1.5|\n",
      "           |  2|6.0|\n",
      "           +---+---+\n",
      "           >>> @pandas_udf(\n",
      "           ...    \"id long, `ceil(v / 2)` long, v double\",\n",
      "           ...    PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
      "           >>> def sum_udf(key, pdf):\n",
      "           ...     # key is a tuple of two numpy.int64s, which is the values\n",
      "           ...     # of 'id' and 'ceil(df.v / 2)' for the current group\n",
      "           ...     return pd.DataFrame([key + (pdf.v.sum(),)])\n",
      "           >>> df.groupby(df.id, ceil(df.v / 2)).apply(sum_udf).show()  # doctest: +SKIP\n",
      "           +---+-----------+----+\n",
      "           | id|ceil(v / 2)|   v|\n",
      "           +---+-----------+----+\n",
      "           |  2|          5|10.0|\n",
      "           |  1|          1| 3.0|\n",
      "           |  2|          3| 5.0|\n",
      "           |  2|          2| 3.0|\n",
      "           +---+-----------+----+\n",
      "        \n",
      "           .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is\n",
      "               recommended to explicitly index the columns by name to ensure the positions are correct,\n",
      "               or alternatively use an `OrderedDict`.\n",
      "               For example, `pd.DataFrame({'id': ids, 'a': data}, columns=['id', 'a'])` or\n",
      "               `pd.DataFrame(OrderedDict([('id', ids), ('a', data)]))`.\n",
      "        \n",
      "           .. seealso:: :meth:`pyspark.sql.GroupedData.apply`\n",
      "        \n",
      "        3. GROUPED_AGG\n",
      "        \n",
      "           A grouped aggregate UDF defines a transformation: One or more `pandas.Series` -> A scalar\n",
      "           The `returnType` should be a primitive data type, e.g., :class:`DoubleType`.\n",
      "           The returned scalar can be either a python primitive type, e.g., `int` or `float`\n",
      "           or a numpy data type, e.g., `numpy.int64` or `numpy.float64`.\n",
      "        \n",
      "           :class:`MapType` and :class:`StructType` are currently not supported as output types.\n",
      "        \n",
      "           Group aggregate UDFs are used with :meth:`pyspark.sql.GroupedData.agg` and\n",
      "           :class:`pyspark.sql.Window`\n",
      "        \n",
      "           This example shows using grouped aggregated UDFs with groupby:\n",
      "        \n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> df = spark.createDataFrame(\n",
      "           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "           ...     (\"id\", \"v\"))\n",
      "           >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n",
      "           ... def mean_udf(v):\n",
      "           ...     return v.mean()\n",
      "           >>> df.groupby(\"id\").agg(mean_udf(df['v'])).show()  # doctest: +SKIP\n",
      "           +---+-----------+\n",
      "           | id|mean_udf(v)|\n",
      "           +---+-----------+\n",
      "           |  1|        1.5|\n",
      "           |  2|        6.0|\n",
      "           +---+-----------+\n",
      "        \n",
      "           This example shows using grouped aggregated UDFs as window functions. Note that only\n",
      "           unbounded window frame is supported at the moment:\n",
      "        \n",
      "           >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
      "           >>> from pyspark.sql import Window\n",
      "           >>> df = spark.createDataFrame(\n",
      "           ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
      "           ...     (\"id\", \"v\"))\n",
      "           >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n",
      "           ... def mean_udf(v):\n",
      "           ...     return v.mean()\n",
      "           >>> w = Window \\\n",
      "           ...     .partitionBy('id') \\\n",
      "           ...     .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
      "           >>> df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()  # doctest: +SKIP\n",
      "           +---+----+------+\n",
      "           | id|   v|mean_v|\n",
      "           +---+----+------+\n",
      "           |  1| 1.0|   1.5|\n",
      "           |  1| 2.0|   1.5|\n",
      "           |  2| 3.0|   6.0|\n",
      "           |  2| 5.0|   6.0|\n",
      "           |  2|10.0|   6.0|\n",
      "           +---+----+------+\n",
      "        \n",
      "           .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`\n",
      "        \n",
      "        .. note:: The user-defined functions are considered deterministic by default. Due to\n",
      "            optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "            more times than it is present in the query. If your function is not deterministic, call\n",
      "            `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> @pandas_udf('double', PandasUDFType.SCALAR)  # doctest: +SKIP\n",
      "        ... def random(v):\n",
      "        ...     import numpy as np\n",
      "        ...     import pandas as pd\n",
      "        ...     return pd.Series(np.random.randn(len(v))\n",
      "        >>> random = random.asNondeterministic()  # doctest: +SKIP\n",
      "        \n",
      "        .. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
      "            in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "            can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        .. note:: The user-defined functions do not take keyword arguments on the calling side.\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    percent_rank()\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    posexplode(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    posexplode_outer(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|null|null| null|\n",
      "        |  3|      null|null|null| null|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|[x -> 1.0]|   0| foo|\n",
      "        |  1|[x -> 1.0]|   1| bar|\n",
      "        |  2|        []|null|null|\n",
      "        |  3|      null|null|null|\n",
      "        +---+----------+----+----+\n",
      "        \n",
      "        .. versionadded:: 2.3\n",
      "    \n",
      "    pow(col1, col2)\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    quarter(col)\n",
      "        Extract the quarter of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    radians(col)\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        :param col: angle in degrees\n",
      "        :return: angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    rand(seed=None)\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        uniformly distributed in [0.0, 1.0).\n",
      "        \n",
      "        .. note:: The function is non-deterministic in general case.\n",
      "        \n",
      "        >>> df.withColumn('rand', rand(seed=42) * 3).collect()\n",
      "        [Row(age=2, name='Alice', rand=1.1568609015300986),\n",
      "         Row(age=5, name='Bob', rand=1.403379671529166)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    randn(seed=None)\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. note:: The function is non-deterministic in general case.\n",
      "        \n",
      "        >>> df.withColumn('randn', randn(seed=42)).collect()\n",
      "        [Row(age=2, name='Alice', randn=-0.7556247885860078),\n",
      "        Row(age=5, name='Bob', randn=-0.0861619008451133)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    rank()\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    regexp_extract(str, pattern, idx)\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    regexp_replace(str, pattern, replacement)\n",
      "        Replace all substrings of the specified string value that match regexp with rep.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    repeat(col, n)\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    reverse(col)\n",
      "        Collection function: returns a reversed string or an array with reverse order of elements.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('s')).collect()\n",
      "        [Row(s='LQS krapS')]\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('r')).collect()\n",
      "        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rint(col)\n",
      "        Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    round(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    row_number()\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    rpad(col, len, pad)\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rtrim(col)\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    schema_of_json(json)\n",
      "        Parses a JSON string and infers its schema in DDL format.\n",
      "        \n",
      "        :param json: a JSON string or a string literal containing a JSON string.\n",
      "        \n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_json('{\"a\": 0}').alias(\"json\")).collect()\n",
      "        [Row(json='struct<a:bigint>')]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    second(col)\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sequence(start, stop, step=None)\n",
      "        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n",
      "        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n",
      "        otherwise -1.\n",
      "        \n",
      "        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
      "        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
      "        [Row(r=[-2, -1, 0, 1, 2])]\n",
      "        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
      "        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n",
      "        [Row(r=[4, 2, 0, -2, -4])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    sha1(col)\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha2(col, numBits)\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
      "        >>> digests[0]\n",
      "        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n",
      "        >>> digests[1]\n",
      "        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftLeft(col, numBits)\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRight(col, numBits)\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRightUnsigned(col, numBits)\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shuffle(col)\n",
      "        Collection function: Generates a random permutation of the given array.\n",
      "        \n",
      "        .. note:: The function is non-deterministic.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
      "        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n",
      "        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    signum(col)\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sin(col)\n",
      "        :param col: angle in radians\n",
      "        :return: sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sinh(col)\n",
      "        :param col: hyperbolic angle\n",
      "        :return: hyperbolic sine of the given value,\n",
      "                 as if computed by `java.lang.Math.sinh()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    size(col)\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    skewness(col)\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    slice(x, start, length)\n",
      "        Collection function: returns an array containing  all the elements in `x` from index `start`\n",
      "        (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\n",
      "        \n",
      "        :param x: the array to be sliced\n",
      "        :param start: the starting index\n",
      "        :param length: the length of the slice\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
      "        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n",
      "        [Row(sliced=[2, 3]), Row(sliced=[5])]\n",
      "        \n",
      "        .. versionadded:: 2.4\n",
      "    \n",
      "    sort_array(col, asc=True)\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements. Null elements will be placed at the beginning\n",
      "        of the returned array in ascending order or at the end of the returned array in descending\n",
      "        order.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    soundex(col)\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    spark_partition_id()\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. note:: This is indeterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    split(str, pattern)\n",
      "        Splits str around pattern (pattern is a regular expression).\n",
      "        \n",
      "        .. note:: pattern is a string represent the regular expression.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab12cd',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[0-9]+').alias('s')).collect()\n",
      "        [Row(s=['ab', 'cd'])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sqrt(col)\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    stddev(col)\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_pop(col)\n",
      "        Aggregate function: returns population standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_samp(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    struct(*cols)\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions\n",
      "        \n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    substring(str, pos, len)\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    substring_index(str, delim, count)\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sum(col)\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    sumDistinct(col)\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    tan(col)\n",
      "        :param col: angle in radians\n",
      "        :return: tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    tanh(col)\n",
      "        :param col: hyperbolic angle\n",
      "        :return: hyperbolic tangent of the given value,\n",
      "                 as if computed by `java.lang.Math.tanh()`\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toDegrees(col)\n",
      "        .. note:: Deprecated in 2.1, use :func:`degrees` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toRadians(col)\n",
      "        .. note:: Deprecated in 2.1, use :func:`radians` instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    to_date(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted (equivalent to ``col.cast(\"date\")``).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_json(col, options={})\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        :param col: name of column containing a struct, an array or a map.\n",
      "        :param options: options to control converting. accepts the same options as the JSON datasource\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(name='Alice', age=2))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(name='Alice', age=2), Row(name='Bob', age=3)])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, [\"Alice\", \"Bob\"])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[\"Alice\",\"Bob\"]')]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    to_timestamp(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted (equivalent to ``col.cast(\"timestamp\")``).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_utc_timestamp(timestamp, tz)\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n",
      "        timezone, and renders that timestamp as a timestamp in UTC.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n",
      "        timezone to UTC timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        :param timestamp: the column that contains timestamps\n",
      "        :param tz: a string that has the ID of timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc\n",
      "        \n",
      "        .. versionchanged:: 2.4\n",
      "           `tz` can take a :class:`Column` containing timezone ID strings.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    translate(srcCol, matching, replace)\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        The translate will happen when any character in the string matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trim(col)\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trunc(date, format)\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        :param format: 'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    udf(f=None, returnType=StringType)\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. note:: The user-defined functions are considered deterministic by default. Due to\n",
      "            optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "            more times than it is present in the query. If your function is not deterministic, call\n",
      "            `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        .. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
      "            in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "            can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        .. note:: The user-defined functions do not take keyword arguments on the calling side.\n",
      "        \n",
      "        :param f: python function if used as a standalone function\n",
      "        :param returnType: the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    unbase64(col)\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unhex(col)\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, return null if fail.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    upper(col)\n",
      "        Converts a string column to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    var_pop(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    var_samp(col)\n",
      "        Aggregate function: returns the unbiased sample variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    variance(col)\n",
      "        Aggregate function: alias for var_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    weekofyear(col)\n",
      "        Extract the week number of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    when(condition, value)\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "        \n",
      "        :param condition: a boolean :class:`Column` expression.\n",
      "        :param value: a literal value, or a :class:`Column` expression.\n",
      "        \n",
      "        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=4)]\n",
      "        \n",
      "        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=None)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    year(col)\n",
      "        Extract the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "\n",
      "DATA\n",
      "    __all__ = ['PandasUDFType', 'abs', 'acos', 'add_months', 'approxCountD...\n",
      "\n",
      "FILE\n",
      "    c:\\spark\\spark-2.4.8-bin-hadoop2.7\\python\\pyspark\\sql\\functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#usando o help do pyspark\n",
    "help(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05802923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
